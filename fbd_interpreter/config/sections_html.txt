#COMMUN
Les "partial dependency plots" (PDP), les "ICE Plots" et les "ALE Plots" sont un moyen de quantifier l'impact d'une (ou deux) features sur un modèle de machine learning. Ils permettent en effet de tracer, feature par feature, l'influence d'une variation de cette feature sur le modèle.
Il est préférable d'avoir un modèle de bonne qualité avant de lancer ces plots. En effet, il n'y a pas grand intérêt à expliquer un modèle peu fiable. Cependant, ces plots peuvent être tracés indifféremment à partir de données d’entraînement, de test ou même de données non labellisées; la seule condition étant que ces données soit représentatives de la distribution normale des données.
Dans le cas de features corrélées, il est conseillé d'utiliser les ALE plots au lieu des PDP.
Add plots explanation

#PDP
Les plots de dépendance partielle (PDP) montrent l'effet marginal qu’a une ou deux features sur les prévisions d’un modèle de machine learning. Un PDP est la moyenne des lignes d'un tracé ICE. 
En pratique, il est conseillé d'utiliser les PDP et les ICE plots ensemble pour une meilleure interprétation.
Add plots explanation


#ICE
Un tracé ICE (Individual Conditional Expectation) visualise la dépendance de la prédiction sur une feature pour chaque observation séparément, ce qui donne une courbe par instance, contrairement à une courbe globale dans les tracés de dépendance partielle. Un PDP est la moyenne des lignes d'un tracé ICE.
En pratique, il est conseillé d'utiliser les PDP et les ICE plots ensemble pour une meilleure interprétation.
Add plots explanation

#ALE
Les ALE (Accumulated Local Effects) décrivent en moyenne l’influence des features sur la prédiction d’un modèle de Machine Learning.
Ils sont une alternative plus rapide et moins biaisée des plots PDP.
Add plots explanation

#SHAP_GLOBAL
The SHAP values can show how much each predictor contributes, either positively or negatively, to the target variable.
This is like the variable importance plot but it is able to show the positive or negative relationship for each variable with the target

#SHAP_LOCAL
Local interpretability — each observation gets its own set of SHAP values.
This greatly increases its transparency.
We can explain why a case receives its prediction and the contributions of the predictors.
Traditional variable importance algorithms only show the results across the entire population but not on each individual case.
The local interpretability enables us to pinpoint and contrast the impacts of the factors.